{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Medical Sentiment Analysis - Encoder Training on Google Colab\n",
    "\n",
    "This notebook provides a complete setup for training personalized emotion analysis models using transformer-based encoders (XLM-RoBERTa, mDeBERTa) on Google Colab.\n",
    "\n",
    "## Overview\n",
    "- **Task**: 21-dimensional emotion regression for medical text\n",
    "- **Models**: XLM-RoBERTa-base, mDeBERTa-v3-base\n",
    "- **Personalization**: Persona tokens + descriptions\n",
    "- **Languages**: Polish + English\n",
    "\n",
    "## Setup Steps\n",
    "1. Enable GPU runtime (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "2. Run all cells sequentially\n",
    "3. Training will automatically save checkpoints to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## üîß Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu-check"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and system info\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Please enable GPU runtime: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving models and results\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory in Drive\n",
    "import os\n",
    "PROJECT_DIR = '/content/drive/MyDrive/medical_sentiment_analysis'\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "print(f\"Project directory: {PROJECT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "repo-section"
   },
   "source": [
    "## üì• Repository Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/your-username/medical_sentiment_analysis.git\"  # Update with actual repo URL\n",
    "WORK_DIR = \"/content/medical_sentiment_analysis\"\n",
    "\n",
    "# Clone if not exists\n",
    "if not os.path.exists(WORK_DIR):\n",
    "    !git clone {REPO_URL} {WORK_DIR}\n",
    "    print(f\"‚úÖ Repository cloned to {WORK_DIR}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Repository already exists at {WORK_DIR}\")\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Show project structure\n",
    "!find . -type f -name \"*.py\" | head -10\n",
    "!ls -la configs/experiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q uv\n",
    "!uv sync --frozen\n",
    "\n",
    "# Verify key dependencies\n",
    "!python -c \"import transformers, torch, pandas, sklearn, yaml; print('‚úÖ All dependencies installed')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-section"
   },
   "source": [
    "## üìä Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify-data"
   },
   "outputs": [],
   "source": [
    "# Verify data files exist\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_files = [\n",
    "    \"data/processed/base/all.jsonl\",\n",
    "    \"data/processed/encoder/train.jsonl\",\n",
    "    \"data/processed/encoder/val.jsonl\",\n",
    "    \"data/processed/encoder/test.jsonl\",\n",
    "    \"data/personas/personas.json\",\n",
    "    \"artifacts/tokenizers/xlmr-base-with-personas/tokenizer_config.json\",\n",
    "    \"artifacts/tokenizers/mdeberta-v3-base-with-personas/tokenizer_config.json\"\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "for file_path in data_files:\n",
    "    if Path(file_path).exists():\n",
    "        size = Path(file_path).stat().st_size\n",
    "        print(f\"‚úÖ {file_path} ({size:,} bytes)\")\n",
    "    else:\n",
    "        missing_files.append(file_path)\n",
    "        print(f\"‚ùå {file_path} - MISSING\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è Missing {len(missing_files)} files. You may need to:\")\n",
    "    print(\"1. Run data preparation steps locally first\")\n",
    "    print(\"2. Upload missing files to the repository\")\n",
    "    print(\"3. Or run data preparation here (if raw data is available)\")\n",
    "else:\n",
    "    print(\"\\nüéâ All required data files are present!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-preview"
   },
   "outputs": [],
   "source": [
    "# Preview data to verify format\n",
    "!python scripts/preview_base_data.py --config configs/data_base.yaml --sample data/processed/base/sample.jsonl || echo \"Base data preview not available\"\n",
    "\n",
    "# Preview encoder view\n",
    "!ENC_MODE=persona_token python scripts/preview_encoder_view.py --config configs/data_encoder.yaml --sample data/processed/base/sample.jsonl || echo \"Encoder preview not available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-pipeline"
   },
   "outputs": [],
   "source": [
    "# Test the training pipeline components\n",
    "import sys\n",
    "sys.path.append('/content/medical_sentiment_analysis')\n",
    "\n",
    "try:\n",
    "    from modules.models.encoder_regressor import EncoderRegressor\n",
    "    from modules.data.datasets import EncoderJsonlDataset, EncoderDatasetConfig\n",
    "    from modules.training.trainer_encoder import build_trainer\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    print(\"‚úÖ All modules imported successfully\")\n",
    "    \n",
    "    # Test tokenizer loading\n",
    "    tokenizer = AutoTokenizer.from_pretrained('artifacts/tokenizers/xlmr-base-with-personas', use_fast=False)\n",
    "    print(f\"‚úÖ XLM-R tokenizer loaded: {len(tokenizer)} tokens\")\n",
    "    \n",
    "    # Test dataset loading\n",
    "    cfg = EncoderDatasetConfig(max_length=128)\n",
    "    if Path('data/processed/encoder/train.jsonl').exists():\n",
    "        ds = EncoderJsonlDataset('data/processed/encoder/train.jsonl', tokenizer, cfg)\n",
    "        print(f\"‚úÖ Dataset loaded: {len(ds)} samples\")\n",
    "        \n",
    "        # Test one sample\n",
    "        sample = ds[0]\n",
    "        print(f\"‚úÖ Sample shape: input={sample['input_ids'].shape}, labels={sample['labels'].shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Pipeline test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-section"
   },
   "source": [
    "## üöÄ Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-training-dir"
   },
   "outputs": [],
   "source": [
    "# Setup training output directories in Google Drive\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create timestamped training run directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "TRAINING_RUN_DIR = f\"{PROJECT_DIR}/training_runs/{timestamp}\"\n",
    "os.makedirs(TRAINING_RUN_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Training outputs will be saved to: {TRAINING_RUN_DIR}\")\n",
    "\n",
    "# Create symlink for easy access\n",
    "LOCAL_OUTPUT_DIR = \"/content/training_output\"\n",
    "if os.path.exists(LOCAL_OUTPUT_DIR):\n",
    "    !rm -rf {LOCAL_OUTPUT_DIR}\n",
    "!ln -s {TRAINING_RUN_DIR} {LOCAL_OUTPUT_DIR}\n",
    "\n",
    "print(f\"Local training directory: {LOCAL_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-config"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EXPERIMENTS = {\n",
    "    \"xlmr_persona_token\": \"configs/experiment/enc_persona_token_xlmr.yaml\",\n",
    "    \"mdeberta_persona_token\": \"configs/experiment/enc_persona_token_mdeberta.yaml\",\n",
    "    \"xlmr_personalized\": \"configs/experiment/enc_personalized_desc_xlmr.yaml\", \n",
    "    \"mdeberta_personalized\": \"configs/experiment/enc_personalized_desc_mdeberta.yaml\"\n",
    "}\n",
    "\n",
    "# Select experiment to run\n",
    "EXPERIMENT_NAME = \"xlmr_persona_token\"  # Change this to run different experiments\n",
    "CONFIG_PATH = EXPERIMENTS[EXPERIMENT_NAME]\n",
    "\n",
    "print(f\"Selected experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Config file: {CONFIG_PATH}\")\n",
    "\n",
    "# Show config\n",
    "!cat {CONFIG_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "modify-config"
   },
   "outputs": [],
   "source": [
    "# Create modified config for Colab training\n",
    "import yaml\n",
    "import copy\n",
    "\n",
    "# Load original config\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Modify for Colab training\n",
    "colab_config = copy.deepcopy(config)\n",
    "colab_config['training']['output_dir'] = f\"{LOCAL_OUTPUT_DIR}/{EXPERIMENT_NAME}\"\n",
    "colab_config['training']['epochs'] = 3  # Adjust as needed\n",
    "colab_config['training']['train_bs'] = 8  # Adjust based on GPU memory\n",
    "colab_config['training']['eval_bs'] = 16\n",
    "colab_config['training']['logging_steps'] = 25\n",
    "colab_config['training']['evaluation_strategy'] = \"steps\"\n",
    "colab_config['training']['eval_steps'] = 100\n",
    "colab_config['training']['save_strategy'] = \"steps\"\n",
    "colab_config['training']['save_steps'] = 100\n",
    "\n",
    "# Save modified config\n",
    "COLAB_CONFIG_PATH = f\"/tmp/{EXPERIMENT_NAME}_colab.yaml\"\n",
    "with open(COLAB_CONFIG_PATH, 'w') as f:\n",
    "    yaml.dump(colab_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"Modified config saved to: {COLAB_CONFIG_PATH}\")\n",
    "print(f\"Output directory: {colab_config['training']['output_dir']}\")\n",
    "print(f\"Batch sizes: train={colab_config['training']['train_bs']}, eval={colab_config['training']['eval_bs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-training"
   },
   "outputs": [],
   "source": [
    "# Run training\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = '/content/medical_sentiment_analysis'\n",
    "\n",
    "print(f\"üöÄ Starting training: {EXPERIMENT_NAME}\")\n",
    "print(f\"‚è∞ This may take 1-3 hours depending on data size and epochs\")\n",
    "\n",
    "# Run training script\n",
    "!cd /content/medical_sentiment_analysis && python scripts/train_encoder.py --config {COLAB_CONFIG_PATH}\n",
    "\n",
    "print(\"‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitoring-section"
   },
   "source": [
    "## üìà Training Monitoring & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-outputs"
   },
   "outputs": [],
   "source": [
    "# Check training outputs\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = f\"{LOCAL_OUTPUT_DIR}/{EXPERIMENT_NAME}\"\n",
    "\n",
    "if Path(output_dir).exists():\n",
    "    print(f\"üìÅ Training outputs in: {output_dir}\")\n",
    "    !ls -la {output_dir}\n",
    "    \n",
    "    # Check for model checkpoints\n",
    "    checkpoints = list(Path(output_dir).glob(\"checkpoint-*\"))\n",
    "    if checkpoints:\n",
    "        print(f\"\\nüéØ Found {len(checkpoints)} checkpoints:\")\n",
    "        for cp in sorted(checkpoints):\n",
    "            print(f\"  - {cp.name}\")\n",
    "    \n",
    "    # Check training logs\n",
    "    if Path(f\"{output_dir}/trainer_state.json\").exists():\n",
    "        print(\"\\nüìä Training state saved\")\n",
    "        !head -20 {output_dir}/trainer_state.json\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå No outputs found at: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-training"
   },
   "outputs": [],
   "source": [
    "# Plot training metrics if available\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "trainer_state_path = Path(f\"{LOCAL_OUTPUT_DIR}/{EXPERIMENT_NAME}/trainer_state.json\")\n",
    "\n",
    "if trainer_state_path.exists():\n",
    "    with open(trainer_state_path, 'r') as f:\n",
    "        trainer_state = json.load(f)\n",
    "    \n",
    "    # Extract log history\n",
    "    log_history = trainer_state.get('log_history', [])\n",
    "    \n",
    "    if log_history:\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(log_history)\n",
    "        \n",
    "        # Plot training metrics\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(f'Training Metrics: {EXPERIMENT_NAME}', fontsize=16)\n",
    "        \n",
    "        # Training loss\n",
    "        if 'train_loss' in df.columns:\n",
    "            train_df = df.dropna(subset=['train_loss'])\n",
    "            axes[0,0].plot(train_df['step'], train_df['train_loss'], 'b-', label='Train Loss')\n",
    "            axes[0,0].set_title('Training Loss')\n",
    "            axes[0,0].set_xlabel('Step')\n",
    "            axes[0,0].set_ylabel('Loss')\n",
    "            axes[0,0].grid(True)\n",
    "        \n",
    "        # Validation metrics\n",
    "        eval_df = df.dropna(subset=['eval_loss']) if 'eval_loss' in df.columns else pd.DataFrame()\n",
    "        \n",
    "        if not eval_df.empty:\n",
    "            # Validation loss\n",
    "            axes[0,1].plot(eval_df['step'], eval_df['eval_loss'], 'r-', label='Eval Loss')\n",
    "            axes[0,1].set_title('Validation Loss')\n",
    "            axes[0,1].set_xlabel('Step')\n",
    "            axes[0,1].set_ylabel('Loss')\n",
    "            axes[0,1].grid(True)\n",
    "            \n",
    "            # R¬≤ score\n",
    "            if 'eval_r2' in eval_df.columns:\n",
    "                axes[1,0].plot(eval_df['step'], eval_df['eval_r2'], 'g-', label='R¬≤')\n",
    "                axes[1,0].set_title('R¬≤ Score')\n",
    "                axes[1,0].set_xlabel('Step')\n",
    "                axes[1,0].set_ylabel('R¬≤')\n",
    "                axes[1,0].grid(True)\n",
    "            \n",
    "            # MAE\n",
    "            if 'eval_mae' in eval_df.columns:\n",
    "                axes[1,1].plot(eval_df['step'], eval_df['eval_mae'], 'm-', label='MAE')\n",
    "                axes[1,1].set_title('Mean Absolute Error')\n",
    "                axes[1,1].set_xlabel('Step')\n",
    "                axes[1,1].set_ylabel('MAE')\n",
    "                axes[1,1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print final metrics\n",
    "        if not eval_df.empty:\n",
    "            final_metrics = eval_df.iloc[-1]\n",
    "            print(\"\\nüéØ Final Validation Metrics:\")\n",
    "            for col in final_metrics.index:\n",
    "                if col.startswith('eval_') and col != 'eval_runtime':\n",
    "                    print(f\"  {col}: {final_metrics[col]:.4f}\")\n",
    "    else:\n",
    "        print(\"No training logs found\")\n",
    "else:\n",
    "    print(\"No trainer state file found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-evaluation"
   },
   "outputs": [],
   "source": [
    "# Run evaluation on test set\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Find best checkpoint\n",
    "output_dir = f\"{LOCAL_OUTPUT_DIR}/{EXPERIMENT_NAME}\"\n",
    "checkpoint_dirs = list(Path(output_dir).glob(\"checkpoint-*\"))\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    # Use the last checkpoint (highest step number)\n",
    "    best_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.name.split('-')[1]))\n",
    "    print(f\"Using checkpoint: {best_checkpoint}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    eval_cmd = f\"cd /content/medical_sentiment_analysis && EVAL_SPLIT=test python scripts/eval_encoder.py --config {COLAB_CONFIG_PATH} --checkpoint {best_checkpoint}\"\n",
    "    print(f\"Running: {eval_cmd}\")\n",
    "    \n",
    "    !{eval_cmd}\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No checkpoints found for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results-section"
   },
   "source": [
    "## üíæ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-results"
   },
   "outputs": [],
   "source": [
    "# Create results summary\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "results_summary = {\n",
    "    \"experiment_name\": EXPERIMENT_NAME,\n",
    "    \"timestamp\": timestamp,\n",
    "    \"config_path\": CONFIG_PATH,\n",
    "    \"output_directory\": output_dir,\n",
    "    \"colab_session\": True,\n",
    "    \"gpu_info\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\"\n",
    "}\n",
    "\n",
    "# Add final metrics if available\n",
    "trainer_state_path = Path(f\"{output_dir}/trainer_state.json\")\n",
    "if trainer_state_path.exists():\n",
    "    with open(trainer_state_path, 'r') as f:\n",
    "        trainer_state = json.load(f)\n",
    "    \n",
    "    log_history = trainer_state.get('log_history', [])\n",
    "    if log_history:\n",
    "        # Get final evaluation metrics\n",
    "        eval_logs = [log for log in log_history if any(k.startswith('eval_') for k in log.keys())]\n",
    "        if eval_logs:\n",
    "            final_eval = eval_logs[-1]\n",
    "            results_summary['final_metrics'] = {\n",
    "                k: v for k, v in final_eval.items() \n",
    "                if k.startswith('eval_') and k not in ['eval_runtime', 'eval_samples_per_second']\n",
    "            }\n",
    "\n",
    "# Save results summary\n",
    "summary_path = f\"{TRAINING_RUN_DIR}/results_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"üìù Results summary saved to: {summary_path}\")\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(json.dumps(results_summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-models"
   },
   "outputs": [],
   "source": [
    "# Compress and download best model (optional)\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = f\"{LOCAL_OUTPUT_DIR}/{EXPERIMENT_NAME}\"\n",
    "if Path(output_dir).exists():\n",
    "    # Create compressed archive\n",
    "    archive_name = f\"{EXPERIMENT_NAME}_{timestamp}.tar.gz\"\n",
    "    archive_path = f\"{TRAINING_RUN_DIR}/{archive_name}\"\n",
    "    \n",
    "    !tar -czf {archive_path} -C {LOCAL_OUTPUT_DIR} {EXPERIMENT_NAME}\n",
    "    \n",
    "    print(f\"üì¶ Model archived to: {archive_path}\")\n",
    "    \n",
    "    # Show file size\n",
    "    size_mb = Path(archive_path).stat().st_size / 1024 / 1024\n",
    "    print(f\"üìè Archive size: {size_mb:.1f} MB\")\n",
    "    \n",
    "    # Option to download (uncomment if needed)\n",
    "    # from google.colab import files\n",
    "    # files.download(archive_path)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No model outputs to archive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next-steps"
   },
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "1. **Run Other Experiments**: Change `EXPERIMENT_NAME` variable and re-run training cells\n",
    "2. **Hyperparameter Tuning**: Modify `colab_config` parameters (learning rate, batch size, etc.)\n",
    "3. **Extended Training**: Increase epochs for better performance\n",
    "4. **Model Analysis**: Use the evaluation results for model comparison\n",
    "5. **Download Models**: Your trained models are saved to Google Drive at: `{PROJECT_DIR}/training_runs/{timestamp}/`\n",
    "\n",
    "## üìö Available Experiments\n",
    "- `xlmr_persona_token`: XLM-RoBERTa with persona tokens\n",
    "- `mdeberta_persona_token`: mDeBERTa with persona tokens\n",
    "- `xlmr_personalized`: XLM-RoBERTa with persona descriptions\n",
    "- `mdeberta_personalized`: mDeBERTa with persona descriptions"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}