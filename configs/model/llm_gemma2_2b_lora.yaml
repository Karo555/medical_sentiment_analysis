# configs/model/llm_gemma2_2b_lora.yaml
# Google Gemma 2 2B with LoRA configuration for medical sentiment analysis

model:
  name_or_path: "google/gemma-2-2b"
  use_flash_attention: true
  torch_dtype: "bfloat16"
  device_map: "auto"
  use_4bit: true  # Enable 4-bit quantization for memory efficiency with LoRA
  use_8bit: false
  trust_remote_code: true

tokenizer:
  path_or_name: "google/gemma-2-2b"
  use_fast: true
  padding_side: "left"

# LoRA configuration
lora:
  enabled: true
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  bias: "none"
  modules_to_save: null