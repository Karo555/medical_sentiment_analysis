# Baseline XLM-R with Class-Weighted Loss for Imbalanced Dataset
seed: 1337

paths:
  encoder_view_dir: data/processed/encoder    # skąd czytamy {train,val,test}.jsonl
  reports_dir: reports/encoder                # gdzie pisać raporty (opcjonalnie)

files:
  train: train.jsonl
  val: val.jsonl
  test: test.jsonl

tokenizer:
  # Using base XLM-R tokenizer for baseline comparison
  path_or_name: xlm-roberta-base
  use_fast: false  # SentencePiece → najlepiej False

model:
  name_or_path: xlm-roberta-base
  out_dim: 18
  dropout: 0.1
  use_fast_pooler: true
  use_binary_classification: true
  # Class weighting configuration
  use_class_weights: true
  class_weights_method: "balanced"        # balanced | inverse | sqrt_inv
  class_weights_smooth: 1.0              # smoothing factor to prevent extreme weights

data:
  max_length: 256
  label_dim: 18
  clamp_labels_to: [0.0, 1.0]
  return_meta: true

collate:
  pad_to_multiple_of: 8
  pass_meta_keys: ["id","lang","persona_id"]

training:
  output_dir: artifacts/models/encoder/enc_baseline_xlmr_weighted
  epochs: 3
  lr: 5.0e-5
  weight_decay: 0.01
  train_bs: 16
  eval_bs: 32
  grad_accum: 1
  warmup_ratio: 0.1
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  load_best_at_end: true
  metric_for_best_model: "f1_score"
  greater_is_better: true
  logging_steps: 50
  fp16: false
  bf16: false
  report_to: ["none"]
  save_total_limit: 2
  num_workers: 2