# Baseline: XLM-R-base, tryb non_personalized (widok ENCODER zbudowany wcześniej)
seed: 1337

paths:
  encoder_view_dir: data/processed/encoder    # skąd czytamy {train,val,test}.jsonl
  reports_dir: reports/encoder                # gdzie pisać raporty (opcjonalnie)

files:
  train: train.jsonl
  val: val.jsonl
  test: test.jsonl

tokenizer:
  # UŻYJ zmodyfikowanego tokenizer-a z dodanymi tokenami (jeśli tryb persona_token/personalized_desc)
  # albo zwykłego HF ID (dla czystego baseline non_personalized).
  path_or_name: artifacts/tokenizers/xlmr-base-with-personas   # lub: xlm-roberta-base
  use_fast: false  # SentencePiece → najlepiej False

model:
  name_or_path: xlm-roberta-base
  out_dim: 18
  dropout: 0.1
  use_fast_pooler: true
  use_binary_classification: true

data:
  max_length: 256
  label_dim: 18
  clamp_labels_to: [0.0, 1.0]
  return_meta: true

collate:
  pad_to_multiple_of: 8
  pass_meta_keys: ["id","lang","persona_id"]

training:
  output_dir: artifacts/models/encoder/enc_baseline_xlmr
  epochs: 3
  lr: 5.0e-5
  weight_decay: 0.01
  train_bs: 16
  eval_bs: 32
  grad_accum: 1
  warmup_ratio: 0.1
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  load_best_at_end: true
  metric_for_best_model: "f1_score"
  greater_is_better: true
  logging_steps: 50
  fp16: false
  bf16: false
  report_to: ["none"]
  save_total_limit: 2
  num_workers: 2
