# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a medical sentiment analysis research project that focuses on personalized emotion analysis using transformer-based models. The system supports multiple languages (Polish and English) and incorporates persona-based personalization for healthcare-related text analysis.

The project implements encoder-based models (XLM-RoBERTa, mDeBERTa) with binary classification heads for emotion prediction (multi-label binary classification). The system has transitioned from 21-dimensional to a reduced dimensionality based on class imbalance analysis.

## Key Architecture

### Data Pipeline
- **Base contract**: `configs/data_base.yaml` defines the canonical data schema with binary emotion labels (0 or 1)
- **Class imbalance analysis**: System includes analysis and handling of underrepresented emotion categories
- **Data processing**: Raw medical opinions are preprocessed with persona information and binary emotion matrices
- **View materialization**: Encoder views are generated from the base data with class weighting support
- **Splits**: Supports language-agnostic (LOLO) and persona-agnostic (LOPO) evaluation strategies

### Model Architecture
- **EncoderClassifier**: `modules/models/encoder_classifier.py` - Transformer backbone + mean pooling + binary classification head
  - **Binary Classification Mode**: Uses BCEWithLogitsLoss for multi-label binary classification
  - **Class weighting**: Support for weighted loss to handle class imbalance
- **Persona handling**: Three modes - `non_personalized`, `persona_token`, `personalized_desc`
- **Tokenization**: Custom tokenizers with persona-specific tokens in `artifacts/tokenizers/`
- **Optimization**: Advanced threshold optimization for improved classification performance

### Training Pipeline
- **Encoder Trainer**: `modules/training/trainer_encoder.py` - HuggingFace Trainer wrapper with binary classification metrics
- **Datasets**: `modules/data/datasets.py` - JSONL-based datasets for encoder views with flexible tokenization
- **Collate Functions**: `modules/data/collate.py` - Specialized batch processing for encoder training
- **Metrics**: `modules/metrics/classification.py` - Binary classification metrics (Accuracy, F1-score, Precision, Recall)
- **Optimization**: `modules/optimization/` - Threshold optimization and class weighting utilities

## Common Development Commands

### Data Preparation
```bash
# Preview data contracts and smoke tests
make preview-base                    # Test base data contract
make preview-enc                     # Preview encoder view (ENC_MODE=persona_token)

# Data preprocessing and splits
make prepare-data                    # Process raw data to base format
make make-splits                     # Generate train/val/test splits

# Build materialized views
make build-enc-view                  # Build encoder view (configurable mode)

# Class imbalance analysis and optimization
python scripts/analyze_class_imbalance.py    # Analyze class distribution
python scripts/remove_underrepresented_labels.py  # Remove low-frequency labels
python scripts/test_class_weights.py        # Test class weighting strategies
python scripts/optimize_thresholds.py       # Optimize classification thresholds
```

### Model Training

#### Encoder Training
```bash
# Baseline encoder training
make train-enc-baseline                  # XLM-R baseline model
make train-enc-baseline-weighted         # XLM-R baseline with class weighting

# Specific model configurations
make train-enc-persona-token-xlmr        # XLM-R with persona tokens
make train-enc-personalized-xlmr         # XLM-R with persona descriptions
make train-enc-persona-token-mdeberta    # mDeBERTa with persona tokens
make train-enc-personalized-mdeberta     # mDeBERTa with persona descriptions
```

### Evaluation and Analysis

#### Encoder Evaluation
```bash
# Model evaluation (default: validation set)
make eval-enc CHECKPOINT=artifacts/models/encoder/model_name

# Test set evaluation
make eval-enc-test CHECKPOINT=artifacts/models/encoder/model_name

# Calibration analysis
make calib-enc CHECKPOINT=artifacts/models/encoder/model_name
```

#### Analysis and Visualization
```bash
# Training visualization
python scripts/visualize_training.py CHECKPOINT=artifacts/models/encoder/model_name

# Performance analysis and comparison
python scripts/plot_calibration.py       # Model calibration analysis
```

### Configuration System
All experiments are defined via YAML configs in `configs/`:
- `experiment/` - Complete experiment configurations for encoder models
- `model/` - Model architecture definitions
- `data_*.yaml` - Data processing contracts for encoder views

#### Key Configuration Variables:
- `ENC_MODE`: `non_personalized` | `persona_token` | `personalized_desc`
- `CHECKPOINT`: Path to model checkpoint for evaluation
- `EVAL_SPLIT`: `val` | `test`
- `USE_WEIGHTED_LOSS`: Enable class weighting for imbalanced datasets

#### Encoder-Specific Configurations:
- **Model Settings**: Transformer architecture, pooling strategies, dropout rates
- **Training Settings**: Learning rates, batch sizes, gradient accumulation, warmup steps
- **Classification Settings**: Number of labels, class weights, threshold optimization

### Project Structure
- `modules/` - Core Python modules (models, data, training, metrics, optimization, utils)
- `scripts/` - Executable scripts for training, evaluation, data processing, analysis
- `configs/` - YAML configuration files
- `data/` - Organized data with raw, processed, and split directories
- `artifacts/` - Model checkpoints, custom tokenizers, class weights, and evaluation results
- `schema/` - JSON schemas for data validation and label management
- `analysis/` - Data analysis notebooks and results

## Development Notes

### Python Environment
- Uses `uv` for dependency management (see `pyproject.toml`)
- Requires Python â‰¥3.10
- Key dependencies: transformers, torch, pandas, scikit-learn, matplotlib, seaborn

### Testing and Validation
- Use `make preview-*` commands for smoke testing before full experiments
- Schema validation is enforced at data processing stages
- All experiments should be reproducible with fixed seeds (default: 1337)

### Multi-language Support
The system is designed for Polish-English bilingual analysis:
- Data contracts specify `lang` field with `pl`/`en` values
- Tokenizers and models handle multilingual input
- Evaluation can be stratified by language

### Persona System
16 healthcare personas are defined in `data/personas/personas.json`:
- Patient personas (young_mother, senior_patient, health_anxiety_patient, etc.)
- Healthcare provider personas (supervising_physician, clinic_manager, etc.)
- Specialized personas (mental_health_advocate, healthcare_journalist, etc.)

### Advanced Classification Features
The project includes comprehensive binary classification capabilities:

#### Class Imbalance Handling
- **Class Analysis**: Automated analysis of label distribution and representation
- **Weighted Loss**: Configurable class weighting to address imbalanced datasets (dramatic improvements: +2,680% F1)
- **Label Reduction**: Tools to remove underrepresented emotion categories
- **Threshold Optimization**: Post-training optimization of classification thresholds per emotion

#### Model Optimization
- **Performance Monitoring**: Detailed training curves and metric visualization
- **Calibration Analysis**: Model confidence calibration assessment
- **Multi-metric Evaluation**: Accuracy, F1-score, Precision, Recall across all labels
- **Cross-validation**: Support for robust model evaluation strategies

## Production Results & Performance

### Model Performance Summary
The system has achieved production-ready performance through weighted loss implementation:

#### XLM-RoBERTa with Weighted Loss (Production Model)
- **Overall F1-Score**: 72.18% (vs 33.74% non-weighted, +114% improvement)
- **Production-Ready Emotions**: 13/18 emotions achieve F1 > 60%
- **Exceptional Performance**: 7/18 emotions achieve F1 > 80%
- **Deployment Status**: Ready for clinical applications

#### mDeBERTa-v3 with Weighted Loss
- **Overall F1-Score**: 43.61% (vs 11.54% non-weighted, +278% improvement)  
- **Status**: Research/development model, requires additional optimization

### Per-Emotion Performance (XLM-RoBERTa Weighted)
**Tier 1 - Exceptional (F1 > 80%)**:
- funny (96.9%), compassion (90.1%), ironic (89.5%), understandable (84.6%), interesting (83.1%), anger (80.7%)

**Tier 2 - Production Ready (F1 60-80%)**:
- political (79.0%), negative (76.3%), fear (73.6%), surprise (74.3%), sadness (72.2%), offensive (69.8%), disgust (69.4%)

**Tier 3 - Development Focus (F1 < 60%)**:
- positive (52.2%), happiness (59.6%), inspiring (46.1%), delight (37.6%), calm (35.5%)

### Class Imbalance Statistics
The dataset exhibits significant class imbalance:
- **Most Balanced**: understandable (1.78:1), interesting (1.13:1), negative (1.60:1)
- **Most Imbalanced**: calm (9.74:1), political (9.56:1), funny (8.78:1)
- **Class Weights Applied**: Range from 0.774 to 5.451 for balanced training

### Language Detection Enhancement
Fixed critical language detection pipeline:
- **Previous**: 1,660 Polish / 8,360 English (incorrect heuristic)
- **Enhanced**: 730 Polish / 9,290 English (improved accuracy)
- **Enhancement**: Expanded vocabulary, pattern detection, robust fallbacks

## Critical Implementation Details

### Model Architecture Specifics
```python
# EncoderClassifier (modules/models/encoder_classifier.py)
class EncoderClassifier(nn.Module):
    def __init__(self, model_name_or_path, out_dim=18, dropout_prob=0.1, 
                 use_fast_pooler=True, use_binary_classification=True,
                 class_weights=None):
        # Transformer backbone (XLM-RoBERTa/mDeBERTa)
        # Mean pooling aggregation
        # Dropout regularization (0.1)
        # Linear classification head (768 â†’ 18)
        # BCEWithLogitsLoss with pos_weight for class weighting
```

### Weighted Loss Configuration
```yaml
# Critical configuration for production performance
model:
  use_class_weights: true
  class_weights_method: "balanced"  # Computes weights as n_samples/(n_classes * counts)
  
training:
  loss_function: "BCEWithLogitsLoss"  # Multi-label binary classification
  pos_weight: "computed_from_class_weights"  # Per-class positive weighting
```

### Threshold Optimization Results
Each emotion requires optimized decision thresholds:
- **Aggressive Detection** (low threshold): funny (0.23), compassion (0.22)
- **Conservative Detection** (high threshold): ironic (0.87), understandable (0.65)
- **Balanced Approach** (mid threshold): anger (0.31), negative (0.34), fear (0.50)

## Deployment Recommendations

### Production Environment Setup
1. **Primary Model**: Use XLM-RoBERTa weighted (`enc_baseline_xlmr_weighted`)
2. **Confidence Thresholds**: Apply emotion-specific optimized thresholds
3. **Monitoring**: Track performance on Tier 3 emotions (< 60% F1)
4. **Language Detection**: Use enhanced heuristic for Polish/English classification

### Performance Monitoring
- **High-Stakes Applications**: Deploy only F1 > 80% emotions (7 emotions)
- **Standard Applications**: Deploy F1 > 60% emotions (13 emotions)
- **Research Applications**: Monitor all 18 emotions with performance tracking

### Future Development Priorities
1. **Data Collection**: Focus on underrepresented emotions (calm, delight, inspiring)
2. **Model Improvements**: Implement focal loss, hierarchical classification
3. **Ensemble Methods**: Combine XLM-RoBERTa + mDeBERTa predictions